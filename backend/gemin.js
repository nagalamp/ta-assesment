import Tesseract from "tesseract.js";
import fs from "fs";
import { GoogleGenAI } from "@google/genai";
import "dotenv/config";

const imagePath = "./table.png";

// Function to extract text with multiple orientations
async function extractTextAllOrientations(imagePath) {
  console.log("ğŸ”„ Extracting text from all orientations...");
  
  const orientations = [
    { psm: Tesseract.PSM.AUTO, name: "Auto" },
    { psm: Tesseract.PSM.AUTO_OSD, name: "Auto OSD" },
    { psm: Tesseract.PSM.SINGLE_BLOCK, name: "Single Block" },
    { psm: Tesseract.PSM.SINGLE_BLOCK_VERT_TEXT, name: "Vertical Text" },
    { psm: Tesseract.PSM.SINGLE_COLUMN, name: "Single Column" },
    { psm: Tesseract.PSM.SINGLE_LINE, name: "Single Line" },
  ];

  let allTexts = [];
  let bestHorizontal = { text: "", confidence: 0 };
  let bestVertical = { text: "", confidence: 0 };

  for (const orientation of orientations) {
    try {
      console.log(`ğŸ”„ Trying ${orientation.name}...`);
      
      const { data } = await Tesseract.recognize(imagePath, "eng", {
        dpi: 300,
        psm: orientation.psm,
        oem: 1,
      });

      console.log(`ğŸ“Š ${orientation.name} - Confidence: ${data.confidence}, Chars: ${data.text.length}`);

      const textData = {
        text: data.text,
        confidence: data.confidence,
        orientation: orientation.name,
        psm: orientation.psm
      };

      allTexts.push(textData);

      // Classify as horizontal or vertical based on PSM mode
      if (orientation.psm === Tesseract.PSM.SINGLE_BLOCK_VERT_TEXT) {
        if (data.confidence > bestVertical.confidence) {
          bestVertical = textData;
        }
      } else {
        if (data.confidence > bestHorizontal.confidence) {
          bestHorizontal = textData;
        }
      }

    } catch (error) {
      console.error(`âŒ ${orientation.name} failed:`, error.message);
    }
  }

  console.log(`âœ… Best Horizontal: ${bestHorizontal.orientation} (conf: ${bestHorizontal.confidence})`);
  console.log(`âœ… Best Vertical: ${bestVertical.orientation} (conf: ${bestVertical.confidence})`);

  // Combine both horizontal and vertical texts
  let combinedText = "";
  
  if (bestHorizontal.text && bestHorizontal.confidence > 0) {
    combinedText += `HORIZONTAL TEXT (${bestHorizontal.orientation}):\n${bestHorizontal.text}\n\n`;
  }
  
  if (bestVertical.text && bestVertical.confidence > 0) {
    combinedText += `VERTICAL TEXT (${bestVertical.orientation}):\n${bestVertical.text}\n\n`;
  }

  // If no good separation, just use all unique text
  if (!combinedText.trim()) {
    const uniqueTexts = new Set();
    allTexts.forEach(item => {
      if (item.text.trim()) {
        uniqueTexts.add(item.text.trim());
      }
    });
    combinedText = Array.from(uniqueTexts).join('\n\n');
  }

  return combinedText;
}

// Manual rotation approach
async function extractWithRotations(imagePath) {
  console.log("ğŸ”„ Extracting with manual rotations...");
  
  let rotationResults = [];
  
  try {
    const sharp = await import('sharp');
    
    const rotations = [
      { angle: 0, name: "Original" },
      { angle: 90, name: "90Â° Clockwise" },
      { angle: 180, name: "180Â°" },
      { angle: 270, name: "90Â° Counter-Clockwise" },
    ];
    
    for (const rotation of rotations) {
      console.log(`ğŸ”„ Rotating ${rotation.name}...`);
      
      let imageBuffer;
      if (rotation.angle === 0) {
        imageBuffer = fs.readFileSync(imagePath);
      } else {
        imageBuffer = await sharp.default(imagePath)
          .rotate(rotation.angle)
          .toBuffer();
      }

      try {
        const { data } = await Tesseract.recognize(imageBuffer, "eng", {
          dpi: 300,
          psm: Tesseract.PSM.AUTO,
          oem: 1,
        });

        rotationResults.push({
          angle: rotation.angle,
          name: rotation.name,
          text: data.text,
          confidence: data.confidence
        });

        console.log(`ğŸ“Š ${rotation.name} - Confidence: ${data.confidence}, Chars: ${data.text.length}`);
        
      } catch (error) {
        console.error(`âŒ ${rotation.name} OCR failed:`, error.message);
      }
    }
    
  } catch (sharpError) {
    console.log("ğŸ“ Sharp not available, skipping rotations");
  }

  // Combine all rotation results
  let rotationText = "";
  rotationResults.forEach(result => {
    if (result.text.trim()) {
      rotationText += `ROTATION ${result.name}:\n${result.text}\n\n`;
    }
  });

  return rotationText;
}

// Main OCR function combining all approaches
async function comprehensiveOCR(imagePath) {
  console.log("ğŸš€ Starting comprehensive OCR...");
  
  let finalText = "";

  // Approach 1: Multiple orientations
  console.log("\n--- APPROACH 1: Multiple Orientations ---");
  const orientationText = await extractTextAllOrientations(imagePath);
  finalText += orientationText;

  // Approach 2: Manual rotations
  console.log("\n--- APPROACH 2: Manual Rotations ---");
  const rotationText = await extractWithRotations(imagePath);
  if (rotationText) {
    finalText += rotationText;
  }

  // Remove duplicates while preserving order
  finalText = removeDuplicateSections(finalText);

  console.log(`\nğŸ“Š FINAL EXTRACTED TEXT: ${finalText.length} characters`);
  
  return finalText;
}

// Helper function to remove duplicate text sections
function removeDuplicateSections(text) {
  const sections = text.split('\n\n');
  const uniqueSections = [];
  const seen = new Set();

  sections.forEach(section => {
    const cleanSection = section.trim();
    if (cleanSection && !seen.has(cleanSection)) {
      seen.add(cleanSection);
      uniqueSections.push(cleanSection);
    }
  });

  return uniqueSections.join('\n\n');
}

// Process with Gemini AI - KEEP ORIGINAL TEXT
async function processWithAI(text) {
  if (!text.trim()) {
    console.error("âŒ No text extracted from image");
    return null;
  }

  console.log("\nğŸ“„ ALL EXTRACTED OCR TEXT:\n", text);
  console.log(`ğŸ“Š Total text length: ${text.length} characters`);

  if (!process.env.GEMINI_API_KEY) {
    console.error("âŒ GEMINI_API_KEY not found in environment variables");
    return null;
  }

  const ai = new GoogleGenAI({ apiKey: process.env.GEMINI_API_KEY });
  const modelName = "gemini-2.5-flash";

  const prompt = `
You are an AI assistant specialized in extracting timetable data from OCR text.

TASK:
- Extract timetable data from the OCR text below
- Keep ALL original text exactly as shown - DO NOT correct, enhance, or modify any words
- Preserve original abbreviations like "Maths" (do NOT change to "Mathematics")
- Preserve original time formats exactly as shown
- Output ONLY valid JSON in this exact format:

{
  "header": {
    "school": "",
    "class": "", 
    "term": "",
    "teacher": ""
  },
  "timetable": [
    {
      "day": "",
      "time": "",
      "subject": ""
    }
  ]
}

CRITICAL RULES:
1. PRESERVE ORIGINAL TEXT: Keep "Maths" as "Maths", "Sci" as "Sci", etc.
2. PRESERVE TIME FORMATS: Keep time exactly as shown "8:00-9:00", "8-9", etc.
3. EXTRACT AS-IS: Do not correct spelling errors or expand abbreviations
4. MULTI-ORIENTATION: The text contains both horizontal and vertical sections
5. COMBINE DATA: Merge information from all orientations into one structured output
6. EMPTY FIELDS: Use empty strings "" for missing information
7. NO MODIFICATIONS: Do not change any text values from the OCR output

OCR TEXT FROM MULTIPLE ORIENTATIONS:
"""${text}"""
`;

  try {
    console.log("ğŸ¤– Processing with Gemini AI (preserving original text)...");
    
    const response = await ai.models.generateContent({
      model: modelName,
      contents: prompt,
    });

    function extractJSON(rawText) {
      try {
        const jsonMatch = rawText.match(/\{[\s\S]*\}/);
        if (jsonMatch) {
          const jsonText = jsonMatch[0];
          const parsed = JSON.parse(jsonText);
          
          // Validate that original text is preserved
          console.log("âœ… JSON parsed successfully, validating original text preservation...");
          return parsed;
        }
        throw new Error("No JSON object found in AI response");
      } catch (e) {
        console.error("âŒ Failed to parse JSON:", e.message);
        console.log("ğŸ” AI RAW OUTPUT:\n", rawText);
        return rawText;
      }
    }

    const jsonOutput = extractJSON(response.text);
    
    if (jsonOutput) {
      console.log("\nâœ… SUCCESS: Structured Data with Original Text:");
      console.log("ğŸ“Š Header Information:");
      console.log(`   School: "${jsonOutput.header.school}"`);
      console.log(`   Class: "${jsonOutput.header.class}"`);
      console.log(`   Term: "${jsonOutput.header.term}"`);
      console.log(`   Teacher: "${jsonOutput.header.teacher}"`);
      
      console.log(`ğŸ“Š Timetable Entries: ${jsonOutput.timetable.length}`);
      jsonOutput.timetable.forEach((entry, index) => {
        console.log(`   ${index + 1}. ${entry.day} | ${entry.time} | ${entry.subject}`);
      });
      
      // Save to file
      const outputFile = "./timetable_output.json";
      fs.writeFileSync(outputFile, JSON.stringify(jsonOutput, null, 2));
      console.log(`ğŸ’¾ Output saved to: ${outputFile}`);
      
      return JSON.stringify(jsonOutput, null, 2);
    } else {
      console.error("âŒ No valid JSON output generated");
      return null;
    }

  } catch (err) {
    console.error("âŒ Gemini AI Error:", err);
    return null;
  }
}

// Alternative: Simple text extraction without AI processing
async function extractRawTextOnly(imagePath) {
  console.log("ğŸ“ Extracting raw text only (no AI processing)...");
  
  const { data } = await Tesseract.recognize(imagePath, "eng", {
    dpi: 300,
    psm: Tesseract.PSM.AUTO,
    oem: 1,
  });

  const rawText = data.text;
  
  console.log("\nğŸ“„ RAW OCR TEXT ONLY:");
  console.log(rawText);
  console.log(`ğŸ“Š Characters: ${rawText.length}`);
  
  // Save raw text to file
  const rawOutputFile = "./raw_ocr_text.txt";
  fs.writeFileSync(rawOutputFile, rawText);
  console.log(`ğŸ’¾ Raw text saved to: ${rawOutputFile}`);
  
  return rawText;
}

// Main function
async function run() {
  if (!fs.existsSync(imagePath)) {
    console.error("âŒ Image file not found:", imagePath);
    console.log("ğŸ’¡ Please ensure 'table.png' exists in the current directory");
    return;
  }

  try {
    console.log("ğŸ¯ Starting timetable extraction...");
    console.log("ğŸ“Œ Mode: Preserve original text (no enhancements)");
    
    // Option 1: Comprehensive extraction with AI processing
    const extractedText = await comprehensiveOCR(imagePath);
    
    if (!extractedText || extractedText.trim().length < 10) {
      console.error("âŒ Very little text extracted, trying simple OCR...");
      await extractRawTextOnly(imagePath);
      return;
    }
    
    // Process with AI while preserving original text
    await processWithAI(extractedText);
    
    console.log("\nâœ… Extraction complete!");
    console.log("ğŸ“‹ Original text preserved - no enhancements applied");
    
  } catch (error) {
    console.error("âŒ Fatal error:", error);
    
    // Final fallback
    try {
      console.log("ğŸ”„ Attempting fallback simple extraction...");
      await extractRawTextOnly(imagePath);
    } catch (fallbackError) {
      console.error("âŒ All extraction methods failed");
    }
  }
}

// Handle uncaught errors
process.on('unhandledRejection', (reason, promise) => {
  console.error('âŒ Unhandled Rejection at:', promise, 'reason:', reason);
});

process.on('uncaughtException', (error) => {
  console.error('âŒ Uncaught Exception:', error);
  process.exit(1);
});

// Run the application
run().catch(console.error);